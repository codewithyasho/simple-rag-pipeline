{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da3a735",
   "metadata": {},
   "source": [
    "## 1.Data ingestion pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9f7eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_classic.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99e69b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the pdfs inside the directory\n",
    "def process_all_pdfs(directory):\n",
    "    '''Process all pdfs in a directory using PyMuPDF'''\n",
    "\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(directory)\n",
    "\n",
    "    # finding all pdfs recursively\n",
    "    pdf_files = list(pdf_dir.glob('**/*.pdf'))\n",
    "\n",
    "    print(f\"\\n====== Found {len(pdf_files)} PDF files to process ======\")\n",
    "\n",
    "    for file in pdf_files:\n",
    "        print(f\"\\nProcessing: {file.name} file\")\n",
    "\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(\n",
    "                str(file)\n",
    "            )\n",
    "            documents = loader.load()\n",
    "\n",
    "            # .extend() adds individual items to the list\n",
    "            all_documents.extend(documents)\n",
    "\n",
    "            print(\n",
    "                f\"\\n‚úÖ Successfully Loaded <{len(documents)}> pages from {file.name}\")\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {file.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n\\nTotal documents loaded: <{len(all_documents)}>\")\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "042bd371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Found 6 PDF files to process ======\n",
      "\n",
      "Processing: Deep Learning 101.pdf file\n",
      "\n",
      "‚úÖ Successfully Loaded <266> pages from Deep Learning 101.pdf\n",
      "==================================================\n",
      "\n",
      "Processing: DeepSeek_OCR_paper.pdf file\n",
      "\n",
      "‚úÖ Successfully Loaded <22> pages from DeepSeek_OCR_paper.pdf\n",
      "==================================================\n",
      "\n",
      "Processing: mathematics-ML.pdf file\n",
      "\n",
      "‚úÖ Successfully Loaded <266> pages from mathematics-ML.pdf\n",
      "==================================================\n",
      "\n",
      "Processing: ML.pdf file\n",
      "\n",
      "‚úÖ Successfully Loaded <169> pages from ML.pdf\n",
      "==================================================\n",
      "\n",
      "Processing: pp_report_1.pdf file\n",
      "\n",
      "‚úÖ Successfully Loaded <14> pages from pp_report_1.pdf\n",
      "==================================================\n",
      "\n",
      "Processing: PP_REPORT_2.pdf file\n",
      "\n",
      "‚úÖ Successfully Loaded <9> pages from PP_REPORT_2.pdf\n",
      "==================================================\n",
      "\n",
      "\n",
      "Total documents loaded: <746>\n"
     ]
    }
   ],
   "source": [
    "all_pdf_docs = process_all_pdfs(\"data/pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c206ff71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'xdvipdfmx (20250205); modified using OpenPDF UNKNOWN', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-15T19:40:49+11:00', 'source': 'data\\\\pdfs\\\\Deep Learning 101.pdf', 'file_path': 'data\\\\pdfs\\\\Deep Learning 101.pdf', 'total_pages': 266, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T22:07:21+05:30', 'trapped': '', 'modDate': \"D:20251015220721+05'30'\", 'creationDate': \"D:20251015194049+11'00'\", 'page': 0}, page_content='')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02aae1",
   "metadata": {},
   "source": [
    "## 2.splitting documents into chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1e8d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d3b9824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(\"\\n‚úÖDocument Chunked successfully!\")\n",
    "    print(\n",
    "        f\"Splitted <{len(documents)}> documents into <{len(chunked_documents)}> chunks.\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0bbd46ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖDocument Chunked successfully!\n",
      "Splitted <746> documents into <1445> chunks.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "chunks = split_docs(all_pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a299515",
   "metadata": {},
   "source": [
    "## 3.Embeddings and vectore store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cefda7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_classic.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "91a17212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store(chunks):\n",
    "\n",
    "    try:\n",
    "        print(\"\\nEmbedding Initiallizing...\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"BAAI/bge-small-en-v1.5\", show_progress=True,\n",
    "            model_kwargs={\n",
    "                'device': 'cpu'\n",
    "            },\n",
    "            encode_kwargs={\n",
    "                'batch_size': 32,\n",
    "                'normalize_embeddings': True\n",
    "            }\n",
    "\n",
    "        )\n",
    "\n",
    "        print(\"\\nVectorStore Initializing...\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Creates a new FAISS index from scratch\n",
    "        vectorstore = FAISS.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embedding_model,\n",
    "            distance_strategy='COSINE'  # Better for normalized embeddings\n",
    "        )\n",
    "\n",
    "        print(\"\\n‚úÖ Embedding and Storing in FAISS Vectorstore successful!\")\n",
    "\n",
    "        print(f\"\\nVector dimension: {vectorstore.index.d}\")\n",
    "        print(f\"üìä Total vectors: {vectorstore.index.ntotal}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Memory usage (approximate)\n",
    "        memory_mb = (vectorstore.index.ntotal *\n",
    "                     vectorstore.index.d * 4) / (1024 * 1024)\n",
    "        print(f\"üíæ Approximate memory: {memory_mb:.2f} MB\")\n",
    "\n",
    "        print(f\"Total Vectors in the store: <{vectorstore.index.ntotal}>\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Save\n",
    "        vectorstore.save_local(\"faiss_index\")\n",
    "        print(\"\\n‚úÖ Successfully saved the FAISS index locally\")\n",
    "\n",
    "        return vectorstore\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during embedding and storing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9de3c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding Initiallizing...\n",
      "==================================================\n",
      "\n",
      "VectorStore Initializing...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e9a36998ae4f619716d4eca0851492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Embedding and Storing in FAISS Vectorstore successful!\n",
      "\n",
      "Vector dimension: 384\n",
      "üìä Total vectors: 1445\n",
      "==================================================\n",
      "üíæ Approximate memory: 2.12 MB\n",
      "Total Vectors in the store: <1445>\n",
      "==================================================\n",
      "\n",
      "‚úÖ Successfully saved the FAISS index locally\n"
     ]
    }
   ],
   "source": [
    "vectorstore = embed_and_store(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b30674",
   "metadata": {},
   "source": [
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ecc0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load later\n",
    "# vectorstore = FAISS.load_local(\"faiss_index\", embeddings=HuggingFaceEmbeddings(\n",
    "#     model=\"BAAI/bge-small-en-v1.5\"), allow_dangerous_deserialization=True)\n",
    "\n",
    "# vectorstore.add_documents(new_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb8292",
   "metadata": {},
   "source": [
    "## 4. create RAG pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "389d025e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12c2f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag(query, vectorstore):\n",
    "    # Retrieve similar documents\n",
    "    similar_docs = vectorstore.similarity_search(\n",
    "        query=query,\n",
    "        k=3\n",
    "    )\n",
    "\n",
    "    # Initialize the Google Generative AI chat model\n",
    "    chat_model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-pro\"\n",
    "    )\n",
    "\n",
    "    # Create a prompt by combining the query with the content of similar documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in similar_docs])\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        '''\n",
    "    Using the following context to answer the question below. \n",
    "    If the context is insufficient, please try to generate the answer based on your own knowledge:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    question: {query}\n",
    "    '''\n",
    "    )\n",
    "\n",
    "    prompt = prompt.format_prompt(\n",
    "        context=context,\n",
    "        query=query\n",
    "    )\n",
    "\n",
    "    # Generate a response using the chat model\n",
    "    response = chat_model.invoke(prompt)\n",
    "\n",
    "    print(context)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fbf8b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831bd0a3952541b8affe1ceab4d3ef5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Intermediate]\n",
      "7.1\n",
      "Parameter Norm Penalties\n",
      "[Intermediate]\n",
      "Parameter norm penalties constrain model capacity by penalizing large weights.\n",
      "7.1.1\n",
      "Intuition: Shrinking the Model‚Äôs ‚ÄùComplexity‚Äù\n",
      "Think of a model as a musical band with many instruments (parameters). If every instrument plays loudly (large weights), the\n",
      "result can be noisy and overfit to the training song. Norm penalties are like asking the band to lower the volume uniformly\n",
      "(L2) or mute many instruments entirely (L1) so the melody (true signal) stands out. This discourages memorization and\n",
      "encourages simpler patterns that generalize.\n",
      "7.1.2\n",
      "L2 Regularization (Weight Decay)\n",
      "Add squared L2 norm of weights to the loss:\n",
      "ÀúL(Œ∏) = L(Œ∏) + Œª\n",
      "2 ‚à•w‚à•2\n",
      "(7.1)\n",
      "91\n",
      "\n",
      "12.2\n",
      "Natural Language Processing\n",
      "[Beginner]\n",
      "12.2.1\n",
      "Text Classification\n",
      "Categorize text documents using pretrained transformers and task headsÕæ fine-tuning is data-efficient and standard\n",
      "Devlin2018Õæ Prince2023Õæ D2LChapterAttention.\n",
      "‚Ä¢ Sentiment analysis: Positive/negative reviews\n",
      "‚Ä¢ Spam detection: Email filtering\n",
      "‚Ä¢ Topic classification: News categorization\n",
      "Models: BERT, RoBERTa, DistilBERTÕæ report accuracy, F1, calibration for risk-sensitive domains.\n",
      "Tokenize +Embed\n",
      "TransformerEncoder (L layers)[CLS] pooled\n",
      "Softmax head\n",
      "Figure 12.4: Transformer fine-tuning for text classification.\n",
      "\n",
      "[14] High-flyer. HAI-LLM: Efficient and lightweight training tool for large models, 2023. URL\n",
      "https://www.high-flyer.cn/en/blog/hai-llm.\n",
      "[15] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S.\n",
      "Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens\n",
      "of generalization. arXiv preprint arXiv:2212.12017, 2022.\n",
      "[16] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. Referitgame: Referring to objects\n",
      "in photographs of natural scenes. In Proceedings of the 2014 conference on empirical\n",
      "methods in natural language processing (EMNLP), pages 787‚Äì798, 2014.\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "query = \"what is llm fine-tuning?\"\n",
    "rag_response = simple_rag(query, vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "50d5e3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context and general knowledge, here is an explanation of LLM fine-tuning:\n",
      "\n",
      "LLM fine-tuning is the process of taking a large, pre-trained language model (like BERT, RoBERTa, or GPT) and further training it on a smaller, task-specific dataset. The goal is to adapt the model's general capabilities to excel at a particular task.\n",
      "\n",
      "From the context:\n",
      "\n",
      "*   **Application:** The context provides a specific example of fine-tuning for **text classification**. It describes a process where a \"pretrained transformer\" is adapted to categorize documents for tasks like sentiment analysis, spam detection, or topic classification.\n",
      "*   **Method:** This is achieved by adding a \"task head\" (e.g., a Softmax head for classification) to the pre-trained model and then continuing the training process.\n",
      "*   **Benefit:** The context highlights that this process is **\"data-efficient and standard,\"** meaning you don't need a massive dataset to achieve good performance, making it a practical approach.\n",
      "\n",
      "In simpler terms, you can think of it like this: A large language model has already completed its \"general education\" by learning language, grammar, and facts from a vast amount of text. Fine-tuning is like giving it specialized job training on a new, smaller set of materials so it can become an expert in a specific area.\n"
     ]
    }
   ],
   "source": [
    "print(rag_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ff25f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
